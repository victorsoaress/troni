# ğŸ“ Troni â€“ Sistema de RecuperaÃ§Ã£o Aumentada por GeraÃ§Ã£o (RAG) para Apoio AcadÃªmico

Este projeto implementa um sistema baseado em **RecuperaÃ§Ã£o Aumentada por GeraÃ§Ã£o (RAG)** para fornecer informaÃ§Ãµes institucionais do curso de **Engenharia MecatrÃ´nica da UFSJ**.  
O sistema combina **modelos de linguagem (LLMs)** com uma base documental curada (arquivos `.pdf` e `.txt`) para garantir respostas mais **precisas, contextualizadas e confiÃ¡veis**, evitando alucinaÃ§Ãµes comuns em LLMs puros.

---

## ğŸš€ Funcionalidades

- ğŸ“š IngestÃ£o automÃ¡tica de documentos acadÃªmicos (PPC, regulamentos, horÃ¡rios, colegiado etc.).
- ğŸ” IndexaÃ§Ã£o vetorial com **Qdrant** para recuperaÃ§Ã£o eficiente.
- ğŸ¤– Respostas baseadas em **RAG** ou **LLM puro** (para comparaÃ§Ã£o).
- ğŸ›¡ï¸ MÃ³dulo de seguranÃ§a com filtros contra *prompt injection*.
- ğŸ“Š Testes com avaliaÃ§Ã£o do sistema com mÃ©tricas quantitativas:
  - Similaridade SemÃ¢ntica
  - ROUGE-L
  - AlucinaÃ§Ã£o
- ğŸ“‚ API em **Django**, com endpoints para perguntas e avaliaÃ§Ã£o.
---
- **Django + DRF** como backend.  
- **Qdrant** como vetor store para busca semÃ¢ntica.  
- **OpenAI Embeddings** para representaÃ§Ã£o vetorial de textos.  
- **LLM (GPT)** para geraÃ§Ã£o de respostas condicionadas ao contexto recuperado.  
- **MÃ©tricas automÃ¡ticas** para avaliar desempenho (similaridade, ROUGE-L, alucinaÃ§Ã£o etc).  
---

## ğŸ—‚ Estrutura dos Arquivos

### `security_prompt.py`
Define mecanismos de **seguranÃ§a contra prompt injection** e **validaÃ§Ã£o de saÃ­da**:
- `PromptInjectionFilter` â†’ detecta padrÃµes perigosos (ex: â€œignore instruÃ§Ãµes anterioresâ€).  
- `OutputValidator` â†’ valida respostas para evitar vazamento de segredos.  
- `SecureLLMPipeline` â†’ combina filtros, contexto e validaÃ§Ã£o antes de chamar a LLM.  

---

### `views.py`
Define a **API principal** para interaÃ§Ã£o com o chatbot (`/api/chat/rag`):
- Recebe `user_query` e `modo` (puro ou RAG).  
- Se **RAG**: recupera categorias, busca chunks no Qdrant, monta contexto e passa para o pipeline seguro.  
- Se **puro**: envia direto para a LLM.  
- Retorna:
  ```json
  {
    "answer": "...",
    "sources": ["arquivo.txt", "documento.pdf"]
  }
### `embeddings_utils.py`
- Gera embeddings com OpenAI (text-embedding-3-large).
- Retorna vetores normalizados para busca semÃ¢ntica.
  
### `qdrant_singleton.py`
Gerencia a conexÃ£o com o Qdrant:
- Implementa Singleton.
- ensure_collection â†’ cria coleÃ§Ãµes ufsj_<categoria>.
- search â†’ busca vetorial.
- upsert_points â†’ insere embeddings com payload (texto, fonte, pÃ¡gina, categoria).

### `extract_chunks.py`
ResponsÃ¡vel por dividir documentos em chunks:
- process_txt â†’ divide .txt em blocos de atÃ© 500 palavras.
- process_pdf â†’ usa pymupdf4llm.to_markdown para extrair PDFs em markdown.
- Cada chunk contÃ©m texto + nÃºmero da pÃ¡gina.

### `ingest_data.py`
Comando Django para ingestÃ£o de dados no Qdrant:
1- LÃª data_config.json â†’ mapeia categorias para arquivos.
2 - Extrai chunks (via extract_chunks).
3 - Gera embeddings (create_embedding).
4 - Insere no Qdrant (upsert_points).

### `tests_pipeline.py`
Script de avaliaÃ§Ã£o do pipeline:
- Executa perguntas nos modos LLM puro e RAG.
- Calcula mÃ©tricas:
-- Similaridade semÃ¢ntica
-- ROUGE-L
-- Taxa de alucinaÃ§Ã£o
-- Fonte recuperada
-- Comprimento da resposta
-Gera avaliacao_completa.json com resultados detalhados.

---

---
ğŸ“Š Fluxo Geral do Sistema

IngestÃ£o de dados â†’ (ingest_data.py) prepara coleÃ§Ãµes no Qdrant.

UsuÃ¡rio faz uma pergunta â†’ (views.py).

Pipeline de seguranÃ§a â†’ (security_prompt.py) aplica filtros e validaÃ§Ãµes.

Busca semÃ¢ntica â†’ (qdrant_singleton.py + embeddings_utils.py).

LLM gera resposta â†’ com base no contexto recuperado.

MÃ©tricas de avaliaÃ§Ã£o â†’ (tests_pipeline.py) validam a qualidade do sistema.
--
ğŸ› ï¸ InstalaÃ§Ã£o
Clonar o RepositÃ³rio
<pre>git clone https://github.com/seu-usuario/troni-rag.git
cd troni-rag</pre>

Criar ambiente virtual
<pre> python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows </pre>

Instalar dependÃªncia
<pre>pip install -r requirements.txt</pre>

Configurar variÃ¡veis de ambiente
<pre>
OPENAI_API_KEY = "sua_api_key"
COHERE_API_KEY = "sua_api_key"
QDRANT_API_KEY = "sua_api_key"
QDRANT_URL = "sua_qdrant_url"
</pre>

Na pasta data, vocÃª pode configurar os seus arquivos da forma como preferir, os arquivos utilizados nesse desenvolvimento estÃ£o disponÃ­veis.

Caso queira modificar, serÃ¡ necessÃ¡ria a reindexaÃ§Ã£o, que vocÃª pode fazer utilizando.
<pre>python manage.py ingest_data</pre>

Caso queira alterar as perguntas de teste, basta realizar no arquivo rag/management/commands/tests_pipeline. Para rodar os testes:
<pre>python manage.py tests_pipeline</pre>

Para ter acesso a um front-end bÃ¡sico, basta rodar o servidor:
<pre>python manage.py runserver</pre>

Caso queria utilizar integrando a outro tipo de desenvolvimento, apenas consumido a API, fazer a requisiÃ§Ã£o para "SUA_URL_LOCAL:SUA_PORTA_LOCAL/api/chat/rag", apÃ³s iniciar o servidor.

